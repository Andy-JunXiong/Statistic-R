---
title: "Untitled"
author: "Andy"
date: "2018/9/19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

## Libraries to load
```{r}
library(gplots)
library(class)
```

## Load data
```{r}
library(mlbench)
data(Sonar)
dim(Sonar)
```

## Partition the data into training and testing data
```{r}
library(caret)
set.seed(1)
inTrain <- createDataPartition(Sonar$Class, p=.5)[[1]]
SonarTrain <- Sonar[ inTrain, ]
SonarTest <- Sonar[ -inTrain, ]
```

## Filter feature selection
# Simple approach: fold change
```{r}
# rowSums=apply(x, 1, sum), rowMeans=apply(x, 1, mean), colSums=apply(x, 2, sum),
# colMeans=apply(x, 2, mean)
# sapply() function the first parameter is the data, and the second parameter is the method, such as sum and mean
SonarTrain.byClass <- split(SonarTrain[, -61], SonarTrain$Class)
feature.mean.byClass <- sapply(SonarTrain.byClass, colMeans)

# calculate fold change of features by class and take the absolute of its log value
feature.foldChange <- abs(log2(feature.mean.byClass[, 1] / feature.mean.byClass[, 2]))

# sort the features by fold change
feature.sorted <- sort(feature.foldChange, decreasing=TRUE)

# select the top 10 features
filtered.features1 <- names(feature.sorted)[1:10]
filtered.features1
```
```{r}
# fitting the classifier on full expression dataset
knn.full <- knn(train=SonarTrain[, -61], test=SonarTest[, -61], cl=SonarTrain$Class, k=5, prob=TRUE)
confusionMatrix(knn.full, SonarTest$Class)
table(knn.full, SonarTest$Class)
```
```{r}
# fitting the classifier on top 10 filtered features
knn.filtered <- knn(train=SonarTrain[, filtered.features1], test=SonarTest[, filtered.features1], cl=SonarTrain$Class, k=5, prob=TRUE)
confusionMatrix(knn.filtered, SonarTest$Class)
table(knn.filtered, SonarTest$Class)
```

## More sophisticated approach based on t-test
```{r}
SonarTrain.byClass <- split(SonarTrain[, -61], SonarTrain$Class)

# perform a t-test
feature.pvalues <- c()
for(i in 1:(ncol(SonarTrain)-1)){
  feature.pvalues <- c(feature.pvalues, t.test(SonarTrain.byClass[[1]][,i], SonarTrain.byClass[[2]][,i])$p.value)
}
names(feature.pvalues) <- colnames(SonarTrain[, -61])

# filter the top 10 most discriminative features based on p-values
filtered.features2 <- names(sort(feature.pvalues)[1:10])

# fitting the classifier on full expression dataset
knn.full <- knn(train=SonarTrain[, -61], test=SonarTest[, -61], cl=SonarTrain$Class, k=5, prob=TRUE)
confusionMatrix(knn.full, SonarTest$Class)
table(knn.full, SonarTest$Class)
```
```{r}
# fitting the classifier using top 10 filtered features by fold change
knn.filtered <- knn(train=SonarTrain[, filtered.features1], test=SonarTest[, filtered.features1], cl=SonarTrain$Class, k=5, prob=TRUE)
confusionMatrix(knn.filtered, SonarTest$Class)
table(knn.filtered, SonarTest$Class)
```
```{r}
knn.filtered <- knn(train=SonarTrain[, filtered.features2], test=SonarTest[, filtered.features2], cl=SonarTrain$Class, k=5, prob=TRUE)
confusionMatrix(knn.filtered, SonarTest$Class)
table(knn.filtered, SonarTest$Class)
```

## Visualise the features selected by filtering step using clustered "heatmap"
```{r}
library(gplots)
classcolors <- sapply(as.character(SonarTrain$Class), switch, R="green3", M="orange3")
SonarFiltered <- t(apply(SonarTrain[, filtered.features2], 2, as.numeric))

heatmap.2(SonarFiltered, col=bluered(75), ColSideColors=classcolors, density.info="none", trace="none", na.color="black", margins=c(8, 8), main="Clustering by top 10 filtered features", dengrogram="column" )
```

## Wrapper feature selection
# Forward stepwise selection
```{r}
selectFeature <- function(train, test, cls.train, cls.test, features){
  ## identify afeature to be selected
  current.best.accuracy <- -Inf
  selected.i <- NULL
  for(i in 1:ncol(train)){
    current.f <- colnames(train)[i]
    if(!current.f %in% features){
      model <- knn(train=train[, c(features, current.f)], test=test[, c(features, current.f)], cl=cls.train, k=3)
      test.acc <- sum(model==cls.test)/length(cls.test)
      
      if(test.acc > current.best.accuracy){
        current.best.accuracy <- test.acc
        selected.i <- colnames(train)[i]
      }  
    }
  }
  return(selected.i)
}
```
```{r}
library(caret)
set.seed(1)
inTrain <- createDataPartition(Sonar$Class, p = .6)[[1]]
allFeatures <- colnames(Sonar)[-61]
train <- Sonar[ inTrain,-61]
test  <- Sonar[-inTrain,-61]
cls.train <- Sonar$Class[inTrain]
cls.test <- Sonar$Class[-inTrain]
```
```{r}
# use correlation to determine the first feature
cls.train.numeric <- rep(c(0, 1), c(sum(cls.train == "R"), sum(cls.train == "M")))
features <- c()
current.best.cor <- 0
for(i in 1:ncol(train[,-61])) {
  if(current.best.cor < abs(cor(train[,i], cls.train.numeric))) {
    current.best.cor <- abs(cor(train[,i], cls.train.numeric))
    features <- colnames(train)[i]
  }
}
print(features)
```
```{r}
# select the 2 to 10 best features using knn as a wrapper classifier
for (j in 2:10) {
  selected.i <- selectFeature(train, test, cls.train, cls.test, features)
  print(selected.i)

  # add the best feature from current run
  features <- c(features, selected.i)
}
```
## Classify on the two types of samples using the full dataset compared to using  top 10 wrapper selected features
```{r}
# fitting the classifier on top 10 wrapper selected features
knn.fit3 <- knn(train=SonarTrain[, features], test=SonarTest[, features], cl=SonarTrain$Class, k=5, prob=TRUE)
confusionMatrix(knn.fit3, SonarTest$Class)
table(knn.fit3, SonarTest$Class)
```

```{r}
acc <- c()
for (i in 1:10){
  features_new <- features[-i]
  knn.fit4 <- knn(train=SonarTrain[, features_new], test=SonarTest[, features_new], cl=SonarTrain$Class, k=5, prob=TRUE)
  cm=as.matrix(table(Actual=SonarTest$Class, Predicted=knn.fit4))
  acc_1 <- sum(diag(cm)/length(SonarTest$Class))
  acc <- c(acc, acc_1)
  print(paste("The features without", features[i],", the accuracy is", acc_1))
}
```

## Ridge and Laso regressions
# Load the example data "Hitters" from "ISLR" package
```{r}
library(ISLR)
```
```{r}
names(Hitters)
```
```{r}
dim(Hitters)
```
```{r}
sum(is.na(Hitters$Salary))
```
```{r}
# remove the instance that contrains missing values
Hitters <- na.omit(Hitters)
dim(Hitters)
```
```{r}
sum(is.na(Hitters))
```
```{r}
# create learning matrix X and regression response variable Y
x <- model.matrix(Salary~., Hitters)[, -1]
y <- Hitters$Salary

# partition the data into training and test sets (50% each)
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- -train
y.test <- y[test]
```

## Ridge regression
# glmnet package implements Ridge and Lasson regressions and anything in between
```{r}
library(glmnet)
```
```{r}
# set the range of lambda values to be tested
grid <- 10^seq(10, -2, length=100)

# alpha is the elastichet mixing parameter with 0 correspond to Ridge regression and 1 correspond to Lasso and anything in between correspond to elastic net
ridge.mod <- glmnet(x[train, ], y[train], alpha=0, lambda=grid)
dim(coef(ridge.mod))

plot(ridge.mod, "lambda", label=TRUE)
```
```{r}
# we can use cross-validataion to determine optimal lambda value. This is implemented as a function in glmnet package.
set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha=0)
plot(cv.out)
```
```{r}
bestlam <- cv.out$lambda.min
bestlam
```
```{r}
# we then predict on the test data using optimal lambda determined by CV
ridge.pred <- predict(ridge.mod, s=bestlam, newx=x[test,])
# and compute the MSE
mean((ridge.pred - y.test)^2)
```
```{r}
# Rigit for feature selections?
ridge.coef <- predict(ridge.mod, type="coefficients", s=bestlam)[1:20, ]
ridge.coef
```
```{r}
which(abs(ridge.coef) > 5)
```

## Lasso regression
```{r}
## Lasso model
lasso.mod <- glmnet(x[train, ], y[train], alpha=1, lambda=grid)
dim(coef(lasso.mod))
plot(lasso.mod, "lambda", label=TRUE)
```
```{r}
set.seed(1)
# Using cross-validation for Lasso to find the best lambda (based on cvm "mena cross=validated error")
cv.lasso <- cv.glmnet(x[train, ], y[train], alpha=1)
plot(cv.lasso)
```
```{r}
bestlam <- cv.lasso$lambda.min
# predict on test set using optimal lambda value estimated by CV
lasso.pred <- predict(lasso.mod, s=bestlam, newx=x[test,])
# compute MSE
mean((lasso.pred - y.test)^2)
```
```{r}
# Lasso for feature selection
lasso.coef=predict(lasso.mod, type="coefficients", s=bestlam)[1:20, ]
lasso.coef
```

